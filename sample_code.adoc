**本文档是为了演示升级到CDP后，应用层面所需的修改**

### Hive
**1. Casting Timestamps**

产生的UTC时间

SELECT CAST(1597217764557 AS TIMESTAMP);
2020-08-12 07:36:04.557

产生当前时区的时间，也就是北京时间

select from_utc_timestamp(1597217764557,'PRC');
2020-08-12 15:36:04.557

2.更正查询中的 `db.table`
错误方式
create table `math.students`(id string)
Error while compiling statement: FAILED: SemanticException line 1:13 Table or database name may not contain dot(.) character 'math.students'

正确方式
create table `math`.`students`(id string);
DESCRIBE `math`.`students`;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+

3.处理关键字APPLICATION
错误方式
create table application (id string);
Error while compiling statement: FAILED: ParseException line 1:13 cannot recognize input near 'application' '(' 'id' in table name

正确方式
create table `application` (id string);

describe extended `application`;

+-----------------------------+----------------------------------------------------+----------+
|          col_name           |                     data_type                      | comment  |
+-----------------------------+----------------------------------------------------+----------+
| id                          | string                                             |          |
|                             | NULL                                               | NULL     |
| Detailed Table Information  | Table(tableName:application, dbName:tpcds_bin_partitioned_orc_20, owner:hive, createTime:1608382275, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null)], location:hdfs://ccycloud-1.cdp-hanjun.root.hwx.site:8020/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_20.db/application, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numRows=0, rawDataSize=0, transactional_properties=default, COLUMN_STATS_ACCURATE={\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"id\":\"true\"}}, numFiles=0, transient_lastDdlTime=1608382275, bucketing_version=2, numFilesErasureCoded=0, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false, catName:hive, ownerType:USER, writeId:0, accessType:8, id:743) |          |


4.处理最大值函数和最小值函数的输出
例如如下的表数据
select * from test;
+------------+------------+
| test.col1  | test.col2  |
+------------+------------+
| 1          | 1          |
| 2          | 1          |
| 3          | NULL       |
+------------+------------+

错误方式
select greatest(col1,col2) from test;
+-------+
|  _c0  |
+-------+
| 1     |
| 2     |
| NULL  |
+-------+

正确方式
SELECT greatest(nvl(col1,0),nvl(col2,0)) FROM test;
+------+
| _c0  |
+------+
| 1    |
| 2    |
| 3    |
+------+

### Impala
1.EXTRACT 和 DATE_PART 函数
select EXTRACT (CAST('2006-05-12 18:27:28.123456789' AS TIMESTAMP), 'MILLISECOND')

| extract(cast('2006-05-12 18:27:28.123456789' as timestamp), 'millisecond') |
+----------------------------------------------------------------------------+
| 28123                                                                      |
+----------------------------------------------------------------------------+

2.SQL 2016 保留关键字

错误方式
select free from test;

Query: select free from test
Query submitted at: 2020-12-27 03:07:09 (Coordinator: http://ccycloud-2.cdp-hanjun.root.hwx.site:25000)
ERROR: ParseException: Syntax error in line 1:
select free from test
       ^
Encountered: A reserved word cannot be used as an identifier: free
Expected: ALL, CASE, CAST, DATE, DEFAULT, DISTINCT, EXISTS, FALSE, IF, INTERVAL, LEFT, NOT, NULL, REPLACE, RIGHT, STRAIGHT_JOIN, TRUNCATE, TRUE, IDENTIFIER

CAUSED BY: Exception: Syntax error


正确方式

select `free` from test;

Query: select `free` from test
Query submitted at: 2020-12-27 03:08:41 (Coordinator: http://ccycloud-2.cdp-hanjun.root.hwx.site:25000)
Query progress can be monitored at: http://ccycloud-2.cdp-hanjun.root.hwx.site:25000/query_plan?query_id=4d4c9be828e87429:1c75513c00000000
+------+
| free |
+------+
| 1    |
+------+
Fetched 1 row(s) in 0.12s

3.别名替换
SELECT ss_item_sk as Item, count(ss_item_sk) as Times_Purchased,sum(ss_quantity) as Total_Quantity_Purchased
FROM store_sales
GROUP BY ss_item_sk
ORDER BY sum(ss_quantity) desc
LIMIT 5;
+-------+-----------------+--------------------------+
| item  | times_purchased | total_quantity_purchased |
+-------+-----------------+--------------------------+
| 21709 | 641             | 33087                    |
| 84901 | 643             | 33028                    |
| 1141  | 655             | 32995                    |
| 11491 | 651             | 32902                    |
| 34981 | 631             | 32448                    |
+-------+-----------------+--------------------------+



### Spark
Spark读取Hive内表需要使用HWC方式，如下所示

如果不使用HWC方式，会报如下的错误：
scala> sql("select * from tpcds_bin_partitioned_orc_20.web_sales").show()
org.apache.spark.sql.AnalysisException:
Spark has no access to table `tpcds_bin_partitioned_orc_20`.`web_sales`. Clients can access this table only if
they have the following capabilities: CONNECTORREAD,HIVEFULLACIDREAD,HIVEFULLACIDWRITE,HIVEMANAGESTATS,HIVECACHEINVALIDATE,CONNECTORWRITE.
This table may be a Hive-managed ACID table, or require some other capability that Spark
currently does not implement;
  at org.apache.spark.sql.catalyst.catalog.CatalogUtils$.throwIfNoAccess(ExternalCatalogUtils.scala:280)
  at org.apache.spark.sql.hive.HiveTranslationLayerCheck$$anonfun$apply$1.applyOrElse(HiveTranslationLayerStrategies.scala:109)
  at org.apache.spark.sql.hive.HiveTranslationLayerCheck$$anonfun$apply$1.applyOrElse(HiveTranslationLayerStrategies.scala:85)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:330)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:328)
  
  
采用HWC的方式如下

##Spark Direct Reader方式
启动spark-shell
spark-shell --jars /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.7.1.5.0-257.jar \
--conf "spark.sql.extensions=com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension" \ --conf "spark.datasource.hive.warehouse.read.via.llap=false" \
--conf "spark.sql.hive.hwc.execution.mode=spark" \
--conf "spark.kryo.registrator=com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator" \
--conf "spark.hadoop.hive.metastore.uris=thrift://172.27.12.128:9083"


同样的执行，结果如下

scala> sql("select * from tpcds_bin_partitioned_orc_20.web_sales").show()
Hive Session ID = 2cb2adc9-32bc-42ec-88d9-1fff4f6e5478
20/12/19 04:28:40 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|ws_sold_time_sk|ws_ship_date_sk|ws_item_sk|ws_bill_customer_sk|ws_bill_cdemo_sk|ws_bill_hdemo_sk|ws_bill_addr_sk|ws_ship_customer_sk|ws_ship_cdemo_sk|ws_ship_hdemo_sk|ws_ship_addr_sk|ws_web_page_sk|ws_web_site_sk|ws_ship_mode_sk|ws_warehouse_sk|ws_promo_sk|ws_order_number|ws_quantity|ws_wholesale_cost|ws_list_price|ws_sales_price|ws_ext_discount_amt|ws_ext_sales_price|ws_ext_wholesale_cost|ws_ext_list_price|ws_ext_tax|ws_coupon_amt|ws_ext_ship_cost|ws_net_paid|ws_net_paid_inc_tax|ws_net_paid_inc_ship|ws_net_paid_inc_ship_tax|ws_net_profit|ws_sold_date_sk|
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|          65319|        2451227|     25246|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|           242|            20|             12|              3|         44|         426395|         65|            49.98|        73.47|         27.91|            2961.40|           1814.15|              3248.70|          4775.55|     90.70|         0.00|          477.10|    1814.15|            1904.85|             2291.25|                 2381.95|     -1434.55|        2451200|
|          65319|        2451202|      6374|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|            52|            13|             13|              4|         35|         426395|         24|            33.46|        84.65|         36.39|            1158.24|            873.36|               803.04|          2031.60|     17.46|         0.00|          893.76|     873.36|             890.82|             1767.12|                 1784.58|        70.32|        2451200|
|          65319|        2451236|     15193|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|           235|            10|              2|              3|        226|         426395|         55|            36.66|        96.78|          7.74|            4897.20|            425.70|              2016.30|          5322.90|      6.13|       221.36|          851.40|     204.34|             210.47|             1055.74|



##JDBC mode 
启动spark-shell
spark-shell --jars /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.7.1.5.0-257.jar \
--conf "spark.sql.extensions=com.hortonworks.spark.sql.rule.Extensions" \
--conf "spark.datasource.hive.warehouse.read.via.llap=false" \
--conf "spark.sql.hive.hwc.execution.mode=spark" \
--conf spark.datasource.hive.warehouse.load.staging.dir=/tmp/ \
--conf spark.datasource.hive.warehouse.read.jdbc.mode=cluster \
--conf spark.sql.hive.hiveserver2.jdbc.url=jdbc:hive2://ccycloud-1.cdp-hanjun.root.hwx.site:10000/default;


执行结果如下

scala> sql("select * from tpcds_bin_partitioned_orc_20.web_sales").show(10)
Hive Session ID = 0c942b4e-7c84-4b0e-bdd2-8121c74357d4
20/12/19 22:25:25 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|ws_sold_time_sk|ws_ship_date_sk|ws_item_sk|ws_bill_customer_sk|ws_bill_cdemo_sk|ws_bill_hdemo_sk|ws_bill_addr_sk|ws_ship_customer_sk|ws_ship_cdemo_sk|ws_ship_hdemo_sk|ws_ship_addr_sk|ws_web_page_sk|ws_web_site_sk|ws_ship_mode_sk|ws_warehouse_sk|ws_promo_sk|ws_order_number|ws_quantity|ws_wholesale_cost|ws_list_price|ws_sales_price|ws_ext_discount_amt|ws_ext_sales_price|ws_ext_wholesale_cost|ws_ext_list_price|ws_ext_tax|ws_coupon_amt|ws_ext_ship_cost|ws_net_paid|ws_net_paid_inc_tax|ws_net_paid_inc_ship|ws_net_paid_inc_ship_tax|ws_net_profit|ws_sold_date_sk|
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|          71262|        2450923|     25249|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           139|            22|             16|              5|         51|         116201|         10|            68.56|       142.60|         91.26|             513.40|            912.60|               685.60|          1426.00|     36.50|         0.00|           42.70|     912.60|             949.10|              955.30|                  991.80|       227.00|        2450816|
|          71262|        2450844|     18277|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           140|            13|              6|              2|        213|         116201|         55|            34.19|        74.19|          5.93|            3754.30|            326.15|              1880.45|          4080.45|      6.52|         0.00|         1223.75|     326.15|             332.67|             1549.90|                 1556.42|     -1554.30|        2450816|
|          71262|        2450819|     26314|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|            28|            22|             16|              3|        127|         116201|         89|            11.57|        17.70|          7.61|             898.01|            677.29|              1029.73|          1575.30|     54.18|         0.00|           31.15|     677.29|             731.47|              708.44|                  762.62|      -352.44|        2450816|
|          71262|        2450856|     10309|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|            74|             1|             18|              5|         82|         116201|         45|            22.04|        55.10|         45.18|             446.40|           2033.10|               991.80|          2479.50|     20.33|         0.00|          793.35|    2033.10|            2053.43|             2826.45|                 2846.78|      1041.30|        2450816|
|          71262|        2450864|     15025|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           175|             2|             16|              5|        107|         116201|         51|            87.40|       186.16|         22.33|            8355.33|           1138.83|              4457.40|          9494.16|     22.32|       819.95|         3512.37|     318.88|             341.20|             3831.25|                 3853.57|     -4138.52|        2450816|
|          71262|        2450851|     13273|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|            19|            20|              5|              1|         15|         116201|         78|            83.05|       210.11|         58.83|           11799.84|           4588.74|              6477.90|         16388.58|    261.55|       229.43|         4260.36|    4359.31|            4620.86|             8619.67|                 8881.22|     -2118.59|        2450816|
|          71262|        2450903|      9656|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           110|            28|              1|              4|        317|         116201|          3|            13.66|        40.43|         36.79|              10.92|            110.37|                40.98|           121.29|      0.00|         0.00|           31.53|     110.37|             110.37|              141.90|                  141.90|        69.39|        2450816|
|          71262|        2450889|     23798|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           122|             7|              2|              5|         98|         116201|        100|            24.91|        25.65|         24.88|              77.00|           2488.00|              2491.00|          2565.00|     26.37|      1169.36|          615.00|    1318.64|            1345.01|             1933.64|                 1960.01|     -1172.36|        2450816|
|          71262|        2450915|      5533|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           100|            28|              6|              4|        100|         116201|         22|            42.76|       109.46|         29.55|            1758.02|            650.10|               940.72|          2408.12|     58.50|         0.00|          914.98|     650.10|             708.60|             1565.08|                 1623.58|      -290.62|        2450816|
|          71262|        2450922|     24914|             173735|         1745974|            4660|          29499|             173735|         1745974|            4660|          29499|           235|            26|              3|              4|        225|         116201|         46|            12.19|        20.96|          6.49|             665.62|            298.54|               560.74|           964.16|      7.76|       143.29|          424.12|     155.25|             163.01|              579.37|                  587.13|      -405.49|        2450816|
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
only showing top 10 rows


##dataframe 方式


import com.hortonworks.hwc.HiveWarehouseSession
import com.hortonworks.hwc.HiveWarehouseSession._

val hive = HiveWarehouseSession.session(spark).build()
hive.setDatabase("tpcds_bin_partitioned_orc_20")
val df = hive.sql("select * from tpcds_bin_partitioned_orc_20.web_sales")
df.createOrReplaceTempView("df_web_sales")
hive.setDatabase("math")
hive.createTable("newTable").ifNotExists().column("ws_sold_time_sk", "bigint").column("ws_ship_date_sk", "bigint").create()
hive.sql("SELECT ws_sold_time_sk, ws_ship_date_sk FROM df_web_sales WHERE ws_sold_time_sk > 80000").write.format("com.hortonworks.spark.sql.hive.llap.HiveWarehouseConnector").mode("append").option("table", "newTable").save()

hive.sql("select * from math.newTable").show(1)
+---------------+---------------+
|ws_sold_time_sk|ws_ship_date_sk|
+---------------+---------------+
|          84134|        2450840|
+---------------+---------------+


