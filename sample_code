##本文档是为了演示升级到CDP后，应用层面所需的修改
### Hive
1. Casting Timestamps
产生的UTC时间
SELECT CAST(1597217764557 AS TIMESTAMP);
2020-08-12 07:36:04.557

产生当前时区的时间，也就是北京时间
select from_utc_timestamp(1597217764557,'PRC');
2020-08-12 15:36:04.557

2.更正查询中的 `db.table`
错误方式
create table `math.students`(id string)
Error while compiling statement: FAILED: SemanticException line 1:13 Table or database name may not contain dot(.) character 'math.students'

正确方式
create table `math`.`students`(id string);
DESCRIBE `math`.`students`;
+-----------+------------+----------+
| col_name  | data_type  | comment  |
+-----------+------------+----------+
| id        | string     |          |
+-----------+------------+----------+

3.处理关键字APPLICATION
错误方式
create table application (id string);
Error while compiling statement: FAILED: ParseException line 1:13 cannot recognize input near 'application' '(' 'id' in table name

正确方式
create table `application` (id string);

describe extended `application`;

+-----------------------------+----------------------------------------------------+----------+
|          col_name           |                     data_type                      | comment  |
+-----------------------------+----------------------------------------------------+----------+
| id                          | string                                             |          |
|                             | NULL                                               | NULL     |
| Detailed Table Information  | Table(tableName:application, dbName:tpcds_bin_partitioned_orc_20, owner:hive, createTime:1608382275, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null)], location:hdfs://ccycloud-1.cdp-hanjun.root.hwx.site:8020/warehouse/tablespace/managed/hive/tpcds_bin_partitioned_orc_20.db/application, inputFormat:org.apache.hadoop.hive.ql.io.orc.OrcInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.ql.io.orc.OrcSerde, parameters:{}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), partitionKeys:[], parameters:{totalSize=0, numRows=0, rawDataSize=0, transactional_properties=default, COLUMN_STATS_ACCURATE={\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"id\":\"true\"}}, numFiles=0, transient_lastDdlTime=1608382275, bucketing_version=2, numFilesErasureCoded=0, transactional=true}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, rewriteEnabled:false, catName:hive, ownerType:USER, writeId:0, accessType:8, id:743) |          |


4.处理最大值函数和最小值函数的输出
例如如下的表数据
select * from test;
+------------+------------+
| test.col1  | test.col2  |
+------------+------------+
| 1          | 1          |
| 2          | 1          |
| 3          | NULL       |
+------------+------------+

错误方式
select greatest(col1,col2) from test;
+-------+
|  _c0  |
+-------+
| 1     |
| 2     |
| NULL  |
+-------+

正确方式
SELECT greatest(nvl(col1,0),nvl(col2,0)) FROM test;
+------+
| _c0  |
+------+
| 1    |
| 2    |
| 3    |
+------+


### Spark
Spark读取Hive内表需要使用HWC方式，如下所示

如果不使用HWC方式，会报如下的错误：

scala> sql("select * from tpcds_bin_partitioned_orc_20.web_sales").show()
org.apache.spark.sql.AnalysisException:
Spark has no access to table `tpcds_bin_partitioned_orc_20`.`web_sales`. Clients can access this table only if
they have the following capabilities: CONNECTORREAD,HIVEFULLACIDREAD,HIVEFULLACIDWRITE,HIVEMANAGESTATS,HIVECACHEINVALIDATE,CONNECTORWRITE.
This table may be a Hive-managed ACID table, or require some other capability that Spark
currently does not implement;
  at org.apache.spark.sql.catalyst.catalog.CatalogUtils$.throwIfNoAccess(ExternalCatalogUtils.scala:280)
  at org.apache.spark.sql.hive.HiveTranslationLayerCheck$$anonfun$apply$1.applyOrElse(HiveTranslationLayerStrategies.scala:109)
  at org.apache.spark.sql.hive.HiveTranslationLayerCheck$$anonfun$apply$1.applyOrElse(HiveTranslationLayerStrategies.scala:85)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$2.apply(AnalysisHelper.scala:108)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:107)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1.apply(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsDown(AnalysisHelper.scala:106)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsDown$1$$anonfun$apply$6.apply(AnalysisHelper.scala:113)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:330)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:328)
  
  
采用HWC的方式如下
启动spark-shell
spark-shell --jars /opt/cloudera/parcels/CDH/lib/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.7.1.5.0-257.jar \
--conf "spark.sql.extensions=com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension" \ --conf "spark.datasource.hive.warehouse.read.via.llap=false" \
--conf "spark.sql.hive.hwc.execution.mode=spark" \
--conf "spark.kryo.registrator=com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator" \
--conf "spark.hadoop.hive.metastore.uris=thrift://172.27.12.128:9083"


同样的执行，结果如下

scala> sql("select * from tpcds_bin_partitioned_orc_20.web_sales").show()
Hive Session ID = 2cb2adc9-32bc-42ec-88d9-1fff4f6e5478
20/12/19 04:28:40 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|ws_sold_time_sk|ws_ship_date_sk|ws_item_sk|ws_bill_customer_sk|ws_bill_cdemo_sk|ws_bill_hdemo_sk|ws_bill_addr_sk|ws_ship_customer_sk|ws_ship_cdemo_sk|ws_ship_hdemo_sk|ws_ship_addr_sk|ws_web_page_sk|ws_web_site_sk|ws_ship_mode_sk|ws_warehouse_sk|ws_promo_sk|ws_order_number|ws_quantity|ws_wholesale_cost|ws_list_price|ws_sales_price|ws_ext_discount_amt|ws_ext_sales_price|ws_ext_wholesale_cost|ws_ext_list_price|ws_ext_tax|ws_coupon_amt|ws_ext_ship_cost|ws_net_paid|ws_net_paid_inc_tax|ws_net_paid_inc_ship|ws_net_paid_inc_ship_tax|ws_net_profit|ws_sold_date_sk|
+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+---------------+
|          65319|        2451227|     25246|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|           242|            20|             12|              3|         44|         426395|         65|            49.98|        73.47|         27.91|            2961.40|           1814.15|              3248.70|          4775.55|     90.70|         0.00|          477.10|    1814.15|            1904.85|             2291.25|                 2381.95|     -1434.55|        2451200|
|          65319|        2451202|      6374|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|            52|            13|             13|              4|         35|         426395|         24|            33.46|        84.65|         36.39|            1158.24|            873.36|               803.04|          2031.60|     17.46|         0.00|          893.76|     873.36|             890.82|             1767.12|                 1784.58|        70.32|        2451200|
|          65319|        2451236|     15193|             174991|         1758861|            1084|         124907|             239584|          151942|            4023|         101893|           235|            10|              2|              3|        226|         426395|         55|            36.66|        96.78|          7.74|            4897.20|            425.70|              2016.30|          5322.90|      6.13|       221.36|          851.40|     204.34|             210.47|             1055.74|



import com.hortonworks.hwc.HiveWarehouseSession
import com.hortonworks.hwc.HiveWarehouseSession._
val hive = HiveWarehouseSession.session(spark).build()
hive.setDatabase("tpcds_bin_partitioned_orc_20")
val df = hive.sql("select * from web_sales")
df.createOrReplaceTempView("web_sales")
hive.setDatabase("testDatabase")
hive.createTable("newTable")
.ifNotExists()
.column("ws_sold_time_sk", "bigint")
.column("ws_ship_date_sk", "bigint")
.create()
sql("SELECT ws_sold_time_sk, ws_ship_date_sk FROM web_sales WHERE ws_sold_time_sk > 80000)
.write.format(HIVE_WAREHOUSE_CONNECTOR)
.mode("append")
.option("table", "newTable")
.save()
