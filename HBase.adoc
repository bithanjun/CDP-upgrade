# HBase sample code

## insert

**错误方式**
```
package main.java.org.hadoop.hbase.hbasetest;
import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.util.Bytes;

public class InsertData {
    public static void main(String[] args) throws IOException {

        // instantiate Configuration class
        Configuration config = HBaseConfiguration.create();

        // instantiate HTable class
        HTable hTable = new HTable(config, "employee");

        // instantiate Put class
        Put p = new Put(Bytes.toBytes("row2001"));
        // add values using add() method
        p.add(Bytes.toBytes("personal"),
                Bytes.toBytes("name"),Bytes.toBytes("Vivek"));
        p.add(Bytes.toBytes("personal"),
                Bytes.toBytes("age"),Bytes.toBytes("17"));
        p.add(Bytes.toBytes("contactinfo"),Bytes.toBytes("city"),
                Bytes.toBytes("Bengaluru"));
        p.add(Bytes.toBytes("contactinfo"),Bytes.toBytes("country"),
                Bytes.toBytes("India"));
        p.add(Bytes.toBytes("contactinfo"),Bytes.toBytes("email"),
                Bytes.toBytes("vivek@abcd.com"));

        // save the put Instance to the HTable.
        hTable.put(p);
        System.out.println("data inserted successfully");

        // close HTable instance
        hTable.close();
    }
}
```

*部分错误信息*

```
Error:(16, 25) java: 无法将类 org.apache.hadoop.hbase.client.HTable中的构造器 HTable应用到给定类型;
  需要: org.apache.hadoop.hbase.client.ConnectionImplementation,org.apache.hadoop.hbase.client.TableBuilderBase,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory,java.util.concurrent.ExecutorService
  找到: org.apache.hadoop.conf.Configuration,java.lang.String
  原因: 实际参数列表和形式参数列表长度不同

Error:(22, 10) java: 对于add(byte[],byte[],byte[]), 找不到合适的方法
    方法 org.apache.hadoop.hbase.client.Mutation.add(org.apache.hadoop.hbase.Cell)不适用
      (实际参数列表和形式参数列表长度不同)
    方法 org.apache.hadoop.hbase.client.Put.add(org.apache.hadoop.hbase.Cell)不适用
      (实际参数列表和形式参数列表长度不同)

```
**正确方式**

```
package main.java.org.hadoop.hbase.hbasetest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.Arrays;
import java.util.stream.Collectors;


public class InsertData {
    private static final TableName TABLE_NAME = TableName.valueOf("employee");
    private static final byte[] CF_NAME = Bytes.toBytes("personal");
    public static void main(String[] args) throws IOException {
        String[] families = new String[] { "personal", "contactinfo" };
        // instantiate Configuration class
        Configuration config = HBaseConfiguration.create();
        try (Connection connection = ConnectionFactory.createConnection(config); Admin admin = connection.getAdmin()) {
            if(!admin.tableExists(TABLE_NAME)) {
                TableDescriptor desc = TableDescriptorBuilder.newBuilder(TABLE_NAME)
                        .setColumnFamilies(Arrays.stream(families)
                                .map(fam -> ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(fam)).build())
                                .collect(Collectors.toList()))
                        .build();
                admin.createTable(desc);
            }

            Put p = new Put(Bytes.toBytes("row2001"));
            p.addColumn(Bytes.toBytes("personal"),
                    Bytes.toBytes("name"),Bytes.toBytes("Vivek"));
            p.addColumn(Bytes.toBytes("personal"),
                    Bytes.toBytes("age"),Bytes.toBytes("17"));
            p.addColumn(Bytes.toBytes("contactinfo"),Bytes.toBytes("city"),
                    Bytes.toBytes("Bengaluru"));
            p.addColumn(Bytes.toBytes("contactinfo"),Bytes.toBytes("country"),
                    Bytes.toBytes("India"));
            p.addColumn(Bytes.toBytes("contactinfo"),Bytes.toBytes("email"),
                    Bytes.toBytes("vivek@abcd.com"));

            try (Table table = connection.getTable(TABLE_NAME)) {
                table.put(p);
            }

        }




        // save the put Instance to the HTable.
//        hTable.put(p);
        System.out.println("data inserted successfully");

        // close HTable instance
//        hTable.close();
    }
}
```


*结果*
```
hbase:008:0> scan 'employee'
ROW                                                              COLUMN+CELL
 row2001                                                         column=contactinfo:city, timestamp=1609583106281, value=Bengaluru
 row2001                                                         column=contactinfo:country, timestamp=1609583106281, value=India
 row2001                                                         column=contactinfo:email, timestamp=1609583106281, value=vivek@abcd.com
 row2001                                                         column=personal:age, timestamp=1609583106281, value=17
 row2001                                                         column=personal:name, timestamp=1609583106281, value=Vivek
1 row(s)
```

## RetriveData

**错误方式**
```
package main.java.org.hadoop.hbase.hbasetest;

import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;

public class RetriveData{

    public static void main(String[] args) throws IOException, Exception{

        Configuration config = HBaseConfiguration.create();
        HTable table = new HTable(config, "employee");

        // instantiate Get class
        Get g = new Get(Bytes.toBytes("row2001"));

        // get the Result object
        Result result = table.get(g);

        // read values from Result class object
        byte [] name = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("name"));
        byte [] age = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("age"));
        byte [] city = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("city"));
        byte [] country = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("country"));
        byte [] email = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("email"));

        System.out.println("name: " + Bytes.toString(name));
        System.out.println("age: " + Bytes.toString(age));
        System.out.println("city: " + Bytes.toString(city));
        System.out.println("country: " + Bytes.toString(country));
        System.out.println("email: " + Bytes.toString(email));
    }
}
```
***错误信息***
```
Error:(16, 24) java: 无法将类 org.apache.hadoop.hbase.client.HTable中的构造器 HTable应用到给定类型;
  需要: org.apache.hadoop.hbase.client.ConnectionImplementation,org.apache.hadoop.hbase.client.TableBuilderBase,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory,java.util.concurrent.ExecutorService
  找到: org.apache.hadoop.conf.Configuration,java.lang.String
  原因: 实际参数列表和形式参数列表长度不同
```

**正确方式**
```
package main.java.org.hadoop.hbase.hbasetest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class RetriveData{

    public static void main(String[] args) throws IOException, Exception{

        Configuration config = HBaseConfiguration.create();
 //       HTable table = new HTable(config, "employee");

        Connection connection = ConnectionFactory.createConnection(config);
        Table table = connection.getTable(TableName.valueOf("employee"));

        // instantiate Get class
        Get g = new Get(Bytes.toBytes("row2001"));

        // get the Result object
        Result result = table.get(g);

        // read values from Result class object
        byte [] name = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("name"));
        byte [] age = result.getValue(Bytes.toBytes("personal"),Bytes.toBytes("age"));
        byte [] city = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("city"));
        byte [] country = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("country"));
        byte [] email = result.getValue(Bytes.toBytes("contactinfo"),Bytes.toBytes("email"));

        System.out.println("name: " + Bytes.toString(name));
        System.out.println("age: " + Bytes.toString(age));
        System.out.println("city: " + Bytes.toString(city));
        System.out.println("country: " + Bytes.toString(country));
        System.out.println("email: " + Bytes.toString(email));
    }
}
```
**结果**
```
21/01/02 18:44:58 INFO zookeeper.ClientCnxn: Session establishment complete on server ccycloud-4.cdp-hanjun.root.hwx.site/172.27.171.6:2181, sessionid = 0x2052d32754a5aa1, negotiated timeout = 60000
name: Vivek
age: 17
city: Bengaluru
country: India
email: vivek@abcd.com
```

## DeleteData ##

**错误方式**

```
package main.java.org.hadoop.hbase.hbasetest;

import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.util.Bytes;

public class DeleteData {

    public static void main(String[] args) throws IOException {

        Configuration conf = HBaseConfiguration.create();
        HTable table = new HTable(conf, "employee");

        // instantiate Delete class
        Delete delete = new Delete(Bytes.toBytes("row2001"));
        delete.deleteColumn(Bytes.toBytes("personal"), Bytes.toBytes("age"));
        delete.deleteFamily(Bytes.toBytes("contactinfo"));

        // delete the data
        table.delete(delete);

        table.close();
        System.out.println("data deleted successfully.....");
    }
}
```
**错误信息**
```
Error:(15, 24) java: 无法将类 org.apache.hadoop.hbase.client.HTable中的构造器 HTable应用到给定类型;
  需要: org.apache.hadoop.hbase.client.ConnectionImplementation,org.apache.hadoop.hbase.client.TableBuilderBase,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory,java.util.concurrent.ExecutorService
  找到: org.apache.hadoop.conf.Configuration,java.lang.String
  原因: 实际参数列表和形式参数列表长度不同
Error:(19, 15) java: 找不到符号
  符号:   方法 deleteColumn(byte[],byte[])
  位置: 类型为org.apache.hadoop.hbase.client.Delete的变量 delete
```

**正确方式**
```
package main.java.org.hadoop.hbase.hbasetest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Delete;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class DeleteData {

    public static void main(String[] args) throws IOException {

        Configuration config = HBaseConfiguration.create();
 //       HTable table = new HTable(conf, "employee");

        Connection connection = ConnectionFactory.createConnection(config);
        Table table = connection.getTable(TableName.valueOf("employee"));

        // instantiate Delete class
        Delete delete = new Delete(Bytes.toBytes("row2001"));
//        delete.deleteColumn(Bytes.toBytes("personal"), Bytes.toBytes("age"));
//        delete.deleteFamily(Bytes.toBytes("contactinfo"));
        delete.addColumn(Bytes.toBytes("personal"), Bytes.toBytes("age"));
        delete.addFamily(Bytes.toBytes("contactinfo"));


        // delete the data
        table.delete(delete);

        table.close();
        System.out.println("data deleted successfully.....");
    }
}
```
**结果**

```
hbase:010:0> scan 'employee'
ROW                                                              COLUMN+CELL
 row2001                                                         column=personal:name, timestamp=1609584266585, value=Vivek
1 row(s)
Took 0.0077 seconds
hbase:011:0>
```

## ScanTable ##

**错误方式**
```
package main.java.org.hadoop.hbase.hbasetest;

import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;

public class ScanTable{

    public static void main(String args[]) throws IOException{

        Configuration config = HBaseConfiguration.create();
        HTable table = new HTable(config, "employee");

        // instantiate the Scan class
        Scan scan = new Scan();

        // scan the columns
        scan.addColumn(Bytes.toBytes("personal"), Bytes.toBytes("name"));
        scan.addColumn(Bytes.toBytes("personal"), Bytes.toBytes("age"));

        // get the ResultScanner
        ResultScanner scanner = table.getScanner(scan);
        for (Result result = scanner.next(); result != null; result=scanner.next())
            System.out.println("Found row : " + result);

        scanner.close();
    }
}

```

**错误信息**
```
Error:(17, 24) java: 无法将类 org.apache.hadoop.hbase.client.HTable中的构造器 HTable应用到给定类型;
  需要: org.apache.hadoop.hbase.client.ConnectionImplementation,org.apache.hadoop.hbase.client.TableBuilderBase,org.apache.hadoop.hbase.client.RpcRetryingCallerFactory,org.apache.hadoop.hbase.ipc.RpcControllerFactory,java.util.concurrent.ExecutorService
  找到: org.apache.hadoop.conf.Configuration,java.lang.String
  原因: 实际参数列表和形式参数列表长度不同
```

**正确方式**
```
package main.java.org.hadoop.hbase.hbasetest;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

public class ScanTable{

    public static void main(String args[]) throws IOException{

        Configuration config = HBaseConfiguration.create();
//        HTable table = new HTable(config, "employee");

        Connection connection = ConnectionFactory.createConnection(config);
        Table table = connection.getTable(TableName.valueOf("employee"));

        // instantiate the Scan class
        Scan scan = new Scan();

        // scan the columns
        scan.addColumn(Bytes.toBytes("personal"), Bytes.toBytes("name"));
        scan.addColumn(Bytes.toBytes("personal"), Bytes.toBytes("age"));

        // get the ResultScanner
        ResultScanner scanner = table.getScanner(scan);
        for (Result result = scanner.next(); result != null; result=scanner.next()) {
            System.out.println("Found row : " + result);
        }

        scanner.close();
    }
}

```

**结果**
```
Found row : keyvalues={row2001/personal:age/1609585694205/Put/vlen=2/seqid=0, row2001/personal:name/1609585694205/Put/vlen=5/seqid=0}
```

## 协处理器 ##
```
package main.java.org.hadoop.hbase.hbasetest;

import org.apache.hadoop.hbase.CoprocessorEnvironment;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.coprocessor.ObserverContext;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessor;
import org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment;
import org.apache.hadoop.hbase.coprocessor.RegionObserver;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.wal.WALEdit;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.logging.Logger;
/**
 * 2.0 版本之前使用extends BaseRegionObserver 实现
 *
 */
public class FollowsObserver implements RegionObserver, RegionCoprocessor {

    private static final Logger LOGGER = (Logger) LoggerFactory.getLogger(FollowsObserver.class);

    private Connection conn;

    private static final TableName TABLE_NAME = TableName.valueOf("test_table_example");
    private static final byte[] CF_NAME = Bytes.toBytes("test_cf");
    private static final byte[] QUALIFIER = Bytes.toBytes("test_column");
    private static final byte[] ROW_ID = Bytes.toBytes("row02");
    @Override
    public void start(CoprocessorEnvironment env) throws IOException {
        LOGGER.info("****** start ******");
        conn = ConnectionFactory.createConnection(env.getConfiguration());
    }

    @Override
    public void stop(CoprocessorEnvironment env) throws IOException {
        LOGGER.info("****** stop ******");
        conn.close();
    }

    public static void putRow(final Table table) throws IOException {
        table.put(new Put(ROW_ID).addColumn(CF_NAME, QUALIFIER, Bytes.toBytes("insert employee!")));
    }

    @Override
    public void postPut(ObserverContext<RegionCoprocessorEnvironment> c, Put put, WALEdit edit, Durability durability)
            throws IOException {


        Table follow_table = conn.getTable(TABLE_NAME);
        putRow(follow_table);

        LOGGER.info("****** insert successfully! ****** ");
    }

}
```
**协处理器jar包加载过程**
```
hbase:002:0> disable 'employee'
Took 0.8609 seconds

hbase:004:0> alter 'employee', METHOD => 'table_att', 'coprocessor' => 'hdfs://172.27.12.128/hbase/hbasetest.jar|main.java.org.hadoop.hbase.hbasetest.FollowsObserver|1001|'
Updating all regions with the new schema...
All regions updated.
Done.

hbase:006:0> enable 'employee'
Took 1.2442 seconds


hbase:007:0> describe 'employee'
Table employee is ENABLED
employee, {TABLE_ATTRIBUTES => {coprocessor$1 => 'hdfs://172.27.12.128/hbase/hbasetest.jar|main.java.org.hadoop.hbase.hbasetest.FollowsObserver|1001|'}}
COLUMN FAMILIES DESCRIPTION
{NAME => 'contactinfo', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_S
COPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}

{NAME => 'personal', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOP
E => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}

```


